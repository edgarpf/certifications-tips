# AWS Certified Data Engineer Associate

* AWS Lake Formation is a service that simplifies the management of security and governance for data lakes. It offers centralized, granular access control to data across multiple AWS services like Amazon S3 and AWS Glue Data Catalog. By using Lake Formation, the administrator can manage permissions in a unified way, which is beneficial for large data lakes with numerous data sources and metadata.
* AWS Glue Studio offers a graphical interface that simplifies the creation, execution, and monitoring of ETL (Extract, Transform, Load) jobs in AWS Glue. This visual interface is especially valuable for teams with varying levels of technical expertise, enabling users to visually compose data transformation workflows.
* The MSCK REPAIR TABLE command in Amazon Athena is specifically designed to address the issue of Athena not returning results from newly added data partitions. This command works by scanning the specified directory in Amazon S3, which is linked to the Athena table, to discover and add any new Hive-compatible partitions. This automatic update of the table metadata in Athena is essential for ensuring that the query engine recognizes and includes newly added data partitions in its query results. The command is particularly beneficial in environments where data partitions in S3 are frequently updated or added, as it streamlines the synchronization process between the data storage in S3 and the metadata in Athena, thereby facilitating accurate and up-to-date query results.
* The AWS Glue FindMatches ML transform, part of the AWS Glue service, is a machine learning tool adept at identifying and associating records that refer to the same entity, especially when a common unique identifier is absent. This process involves a “teaching” approach where users label example records as matches or non-matches. AWS Glue leverages these labeled examples to understand patterns in data, enabling it to recognize similar records.
* Amazon Athena now supports User Defined Functions (UDFs), allowing customers to write custom scalar functions and invoke them in SQL queries. UDFs in Athena are defined within an AWS Lambda function as methods in a Java deployment package. This feature is particularly useful for scenarios requiring custom processing, such as categorizing earthquake locations using a geospatial indexing system. UDFs enable the encapsulation of complex logic into reusable functions, thus simplifying SQL queries in Athena.
* AWS Glue Data Quality, as a component of AWS Glue, automates the identification of data quality issues. It scans the data for common problems such as inconsistencies, missing values, or format errors. This feature is crucial in healthcare scenarios where data must be accurate and consistent. For example, in the case of patient records, AWS Glue Data Quality can automatically detect irregularities like mismatched date formats or incomplete patient entries.
* Amazon Athena’s “Reuse Query Results” feature allows users to reuse the last stored query result when re-running a query, enhancing performance and reducing costs. This is especially beneficial when the underlying data does not change within a certain time frame, like in weekly sales report generation. 
* Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. This feature is highly effective in converting query results into more efficient storage formats, such as Parquet or ORC, which are open source columnar formats. These formats are optimized for Athena, leading to reduced data scanning per query, thereby enhancing query performance and lowering costs. Athena does not support transaction-based operations on table data, ensuring that operations like CREATE, UPDATE, or DELETE are ACID-compliant. When using CTAS, the format property can specify storage formats like ORC, PARQUET, AVRO, JSON, or TEXTFILE, with the default being PARQUET. Additionally, for formats like PARQUET and ORC, compression formats like SNAPPY can be specified using the write_compression property. This approach of transforming data into efficient formats directly caters to the need for improved query performance due to increased data volume and complexity.
* Amazon Athena Federated Query is a powerful feature that allows SQL queries to be executed across multiple data sources without the need for data movement. This service is particularly useful for scenarios where data resides in different storage systems and formats, as it enables querying data in place. With Athena Federated Query, you can extend the scope of your SQL queries to not just Amazon S3 but also to other data sources such as Amazon Aurora MySQL, HBase on Amazon EMR, and Amazon DynamoDB. This capability is crucial for creating comprehensive reports from various data sources without the overhead of extracting, transforming, and loading (ETL) the data into a single repository.
* Amazon Managed Workflows for Apache Airflow is a managed orchestration service for Apache Airflow that you can use to set up and operate data pipelines in the cloud at scale. Apache Airflow is an open-source tool used to programmatically author, schedule, and monitor sequences of processes and tasks referred to as workflows.
* You can use the SSHOperator in a directed acyclic graph (DAG) to connect to a remote Amazon EC2 instance from your Amazon Managed Workflows for Apache Airflow environment. You can also use a similar approach to connect to any remote instance with SSH access. You have to install the apache-airflow-providers-ssh package on the web server via the requirements.txt file.
* Amazon AppFlow is a fully-managed integration service that enables you to securely exchange data between software as a service (SaaS) applications, such as Salesforce, and AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Redshift. For example, you can ingest contact records from Salesforce to Amazon Redshift or pull support tickets from Zendesk to an Amazon S3 bucket.
* Amazon Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3. Parquet and ORC are columnar data formats that save space and enable faster queries compared to row-oriented formats like JSON. 
* Amazon Macie is a security service that utilizes machine learning algorithms to automatically identify, classify, and secure sensitive data in AWS. Amazon Macie protects sensitive data, such as personally identifiable information (PII) (e.g., names, addresses, credit card numbers, and intellectual property), from unauthorized access. After identifying sensitive data, Macie generates detailed reports and alerts, which can be integrated with Amazon EventBridge for automated response actions.
* With AWS Glue, you only pay for the time your ETL job takes to run. You are charged an hourly rate based on the number of Data Processing Units (DPU) to run your ETL job. A Data Processing Unit can also be referred to as a worker, which is the processing power of your ETL job.
* Amazon SageMaker ML Lineage Tracking offers a way to create and store information about the various steps in an ML workflow. This feature is integral for establishing model governance and audit standards, which are essential for ensuring the accuracy, completeness, and trustworthiness of the data used in ML decisions.
* AWS Lake Formation is the optimal choice for setting up a centralized metadata storage solution that demands fine-grained access control. Utilizing Lake Formation data filters, a data engineer can enforce security measures at the database, table, column, row, and cell levels. This capability, coupled with its scalability and reliability, makes Lake Formation the ideal solution with minimal operational overhead.
* In Amazon SQS, several events can lead to the removal of messages, which is crucial for ensuring the queue’s efficiency and reliability. A DeleteMessage API call is a direct method to remove a message from the queue, typically after it has been processed by a consumer. This ensures that messages are not processed more than necessary. Reaching the maxReceiveCount for a message is another way messages are removed; this occurs when a message has been received a specified number of times but not deleted, indicating processing issues and usually results in the message being sent to a dead-letter queue. Furthermore, performing a purge operation on the queue instantly clears all messages, useful for resetting or troubleshooting the queue.
* Substitution is a data-masking technique that involves replacing actual PII data with other authentic-looking values. This technique is useful in maintaining the overall structure and statistical distribution of the data, which is important for testing or development purposes.
* When you select incremental transfer, Amazon AppFlow transfers only the records that have been added or changed since the last successful flow run. This is achieved by selecting a source timestamp field to specify how Amazon AppFlow identifies new or changed records. For example, if you have a Created Date timestamp field, you can instruct Amazon AppFlow to transfer only newly-created records (and not changed records) since the last successful flow run.
* Amazon Managed Streaming for Apache Kafka (Amazon MSK) is a fully managed service that enables you to build and run applications that use Apache Kafka to process streaming data
* Apache Kafka’s Access Control Lists (ACLs) offer a highly granular approach to permissions management. They provide a mechanism to regulate which applications can read or write to a certain topic.
* AWS CloudTrail Lake simplifies activity log analysis by integrating collection, storage, optimization, and query in the same product. By consolidating these features into one environment, CloudTrail Lake eliminates the need for separate data processing pipelines that span across teams and products.
* AWS Glue DataBrew offers a streamlined solution for data quality management, particularly beneficial in scenarios requiring precise and automated data validation. Its ability to define specific data quality rules within a ruleset makes it an optimal choice for scenarios like ensuring inventory data accuracy. 
* Amazon Redshift Spectrum stands out as a highly efficient solution for querying data across Amazon S3 and Amazon Redshift. It leverages the benefits of low-cost storage and open data formats, along with Redshift’s advanced query optimization and fast access to local disks. Redshift Spectrum’s serverless architecture aligns with Amazon Athena’s, negating the need for resource provisioning and management.
* AWS Glue offers a Job run monitoring feature that gives a quick overview of job runs, including status, timestamps, and other key details. For deeper insights, the AWS Glue Job Profiler goes a step further, offering detailed metrics such as execution time, data processing rates, and memory usage. These metrics are crucial for pinpointing performance bottlenecks and fine-tuning resource utilization in ETL jobs.
* Amazon S3 Transfer Acceleration is a bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket. It takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. This results in faster data transfers and reduced latency, which is crucial for the scenario’s requirement of fast access retrieval times.
* Amazon Redshift has two types of nodes: leader nodes and compute nodes.
  * Compute Nodes: These nodes store and process the data. They perform the heavy lifting of running queries, filtering data, and performing computations.
  * Leader Node: This node manages the compute nodes, receives queries, and distributes the workload across the compute nodes. It also manages external communication and aggregates the results from the compute nodes. Certain functions, including current_schema(), are leader node–only functions, meaning they can only be executed on the leader node.
And two types of tables:  
  * System Tables: Redshift has several system tables that store metadata and information about the cluster, databases, schemas, tables, and users. These tables are typically prefixed with “PG_” or “STV_”.
  * User-Defined Tables: These are the tables created by users to store their data in Redshift.
* AWS Database Migration Service(AWS DMS) is a cloud service that makes it possible to migrate relational database, data warehouses, NoSQL databases, and other data stores. You can use AWS DMS to transfer data to Amazon S3 from any database source. When Amazon S3 is employed as a target in an AWS DMS task, both full load and change data capture(CDC) data are written to a comma-separated value (.csv) format by default. However, to ensure more compact storage and faster query performance, you may opt to have the data written in Apache Parquet(.parquet) format.
* The Athena JDBC Connector allows connecting BI tools, reporting tools or custom applications to Amazon Athena using standard JDBC. This enables running SQL queries and fetching results from Athena tables using common SQL tools without needing to use AWS CLI or SDKs.
* AWS Glue FindMatches Machine Learning (ML) Transform enables you to identify duplicate or matching records in your dataset, even when the records do not have a common unique identifier and no fields match exactly. This will not require writing any code or knowing how machine learning works. FindMatches can be useful in many different problems, such as:
  * Matching customers: Linking customer records across different customer databases, even when many customer fields do not match exactly across the databases (e.g., different name spelling, address differences, missing or inaccurate data, etc).
  * Matching products: Matching products in your catalog against other product sources, such as product catalog against a competitor’s catalog, where entries are structured differently.
  * Improving fraud detection: Identifying duplicate customer accounts, determining when a newly created account is (or might be) a match for a previously known fraudulent user.
  * Other matching problems: Match addresses, movies, parts lists, etc. In general, if a human being could look at your database rows and determine that they were a match, there is a really good chance that the FindMatches transform can help you.
* Amazon SageMaker is a service that simplifies building, training, and deploying machine learning models. SageMaker is a platform that facilitates the creation of high-quality machine learning models by abstracting away the complexity typically associated with each process step. It offers an integrated Jupyter Notebook environment for data exploration and preprocessing, optimized and ready-to-use algorithms, and a fully managed deployment service. This enables practitioners to focus more on solving the problem than on the operational overhead of machine learning pipelines.
* SageMaker Canvas is a feature of Amazon SageMaker that represents a no-code approach to machine learning. It makes machine learning accessible to users of all technical backgrounds. With a user-friendly interface, it automates data preparation, model selection, training, and deployment. This allows users to create predictive models without needing to write any code. Canvas simplifies the entire process of applying machine learning, from data cleaning to making predictions.
* The Lock command in Redshift restricts access to the database table. This command enables the acquisition of a table-level lock in “ACCESS EXCLUSIVE” mode. It waits, if necessary, for any conflicting locks to be released. Explicitly locking a table in this way causes other transactions or sessions to wait when attempting to read or write to the table.
* Amazon Redshift has three lock modes:
  * AccessExclusiveLock: Acquired primarily during DDL operations, such as ALTER TABLE, DROP, or TRUNCATE. AccessExclusiveLock blocks all other locking attempts.
  * AccessShareLock: Acquired during UNLOAD, SELECT, UPDATE, or DELETE operations. AccessShareLock blocks only AccessExclusiveLock attempts. AccessShareLock doesn’t block other sessions that are trying to read or write on the table.
  * ShareRowExclusiveLock: Acquired during COPY, INSERT, UPDATE, or DELETE operations. ShareRowExclusiveLock blocks AccessExclusiveLock and other ShareRowExclusiveLock attempts but doesn’t block AccessShareLock attempts.
* By enabling partition projection, Athena can bypass the traditional metadata loading process. Instead, it will directly calculate which partition to access based on the query criteria using rules and patterns specified in the table definition.
* Amazon Athena Federated Query allows you to query data from various sources using AWS Lambda-based connectors. The connectors integrate with data sources like Amazon CloudWatch Logs, DynamoDB, and relational databases like MySQL, enabling SQL queries to span different data locations.
* Amazon AppFlow is a fully managed integration service that enables secure, private data transfer between AWS services and SaaS applications with minimal operational overhead. It allows for the direct integration of SaaS applications like Salesforce, ServiceNow, Slack, and many others with AWS services.
* Amazon Redshift service allows users to create and manage materialized views, which are stored or cached views of a query’s result set for faster access. Redshift supports various automation options, including AWS services and external tools, to automate the updates of these views.
* The query editor v2 in Amazon Redshift can execute SQL commands directly within the Redshift console, including commands to refresh materialized views. These commands can be scheduled to execute automatically and can be set up directly in the Redshift environment. This provides a straightforward method to automate materialized view updates without additional infrastructure or services, thereby minimizing operational overhead.
* AWS Step Functions allows the coordination of multiple AWS services into serverless workflows. By designing a state machine, AWS Step Functions can manage the execution order, dependency tracking, and error handling of the tasks
* The AWS Glue Data Catalog is a fully managed, serverless, and cloud-optimized metadata repository that integrates seamlessly with other AWS services. It provides a unified view of all your data assets, making it easier to discover, search, and manage data across various AWS data stores. AWS Glue crawlers automatically scan your data sources, extract schema information, and populate the Glue Data Catalog with metadata tables, which can be used for data discovery, extract, transform, and load (ETL) jobs, and query execution. This process significantly reduces manual efforts and ensures that your data catalog is always up-to-date with the latest schema changes.
* The AWS Glue Data Catalog provides a centralized metadata repository for clients running Apache Hive and Apache Spark. This serverless repository eliminates the need for operational tasks like scaling, patching, and managing servers, which aligns well with the organization’s goal to adopt a serverless architecture. The AWS Glue Data Catalog can seamlessly integrate with Amazon EMR, providing a managed Hive metastore service that simplifies the process of cataloging, searching, and querying metadata.
* Data distribution in Amazon Redshift refers to how data is stored across the nodes and slices within a cluster. When you load data into a Redshift table, the rows are distributed and stored across these slices according to the table’s distribution style. KEY distribution styles: The rows are distributed based on the values in a specified distribution key column. Rows with the same value are stored together. This helps optimize queries that filter or join on the distribution key column. The different key concepts in Amazon Redshift are:
  * Distribution key: This column is used to distribute the rows across nodes in the cluster. It is specified when creating a table using the DISTKEY clause.
  * Sort key: This column is used to order or sort the rows stored on each node. It is specified using the SORTKEY clause. Multiple columns can be used as sort keys.
  * Primary key: This uniquely identifies each row in the table. It consists of the distribution key as the partition key and can optionally include a sort key.
* The AWS Glue Schema Registry tracks schema changes to tables in the data catalog. It allows schemas to evolve over time while maintaining compatibility with existing analytics, queries, and applications. This enables schema-on-read for analytics tools like Athena, EMR, and Redshift Spectrum.
* AWS CloudTrail’s management events provide visibility into management or control plane operations that are performed on AWS resources, such as creating, modifying, or deleting resources. This includes API calls and console sign-ins. Management events are enabled by default when you create a trail.
* AWS CloudTrail’s data events (also known as “data plane operations”) provide insights into resource or data plane operations performed within resources, such as S3 object uploads and downloads or Lambda function executions. These are often high-volume activities. Data events are disabled by default, and you need to explicitly configure the trail to record them for supported resources. Recording data events incur additional costs.
* AWS Lake Formation is a central hub on Amazon S3 for storing structured and unstructured data until needed. It manages permissions and access for data stored in S3. Lake Formation provides granular control over data access and a unified Storage API for secure data operations. Key features include encryption, private endpoints, and audit logs for enhanced security and isolation of the data lake within S3.
* Converting CSV data into a columnar format such as Apache Parquet significantly improves performance. This conversion enables Athena to process queries more efficiently by reading only the necessary columns, thereby reducing the amount of data scanned and lowering costs. Secondly, ensuring that the AWS region for the S3 data storage matches the region where Athena queries are executed minimizes network latency, leading to faster query performances.
* VACUUM is a command in Amazon Redshift that optimizes query performance by reclaiming disk space from deleted or updated rows, rebuilding indexes, and reorganizing tables. The main VACUUM commands are: 
  * VACUUM FULL – Reclaims disk space, rebuilds indexes, and re-sorts all rows in the table. This is the default VACUUM operation in Redshift.
  * VACUUM DELETE ONLY – Only reclaims disk space occupied by deleted rows without re-sorting data. This is faster than VACUUM FULL but does not optimize query performance.
  * VACUUM REINDEX – Analyzes the distribution of values in interleaved sort key columns and performs a full VACUUM operation, including re-sorting. Used when interleaved sort keys are skewed.
  * VACUUM SORT ONLY – This command performs a sort-only operation on the table without reclaiming disk space. It is useful when rows are unsorted, but space is not an issue.
Amazon Redshift automatically performs VACUUM DELETE ONLY operations in the background to reclaim space from deleted rows.
* You can enable Amazon Athena’s query result caching feature.
* A DataBrew recipe is a set of data transformation steps that are applied sequentially to clean, normalize, and prepare data. Here are some of those recipe steps:
  * NEST_TO_ARRAY converts selected columns into an array while maintaining their order and typecasting them into a common data type.
  * NEST_TO_MAP converts user-selected columns into key-value pairs, with each key representing the column name and the corresponding row value as the value. Column order is not preserved, and different column data types are typecast to a common type.
  * UNNEST_MAP separates a map column into key-value pairs, generating a row for each pair. It only works for one map column level.
  * UNNEST_ARRAY function extracts array elements into separate rows, allowing only one level of an array column to be unnested.
* Amazon Redshift Streaming Ingestion allows ingesting streaming data from Kinesis data streams into an Amazon Redshift data warehouse in near real-time for analytics. It can ingest hundreds of megabytes of data per second with low latency.
* AWS Glue Interactive Sessions provides a programmatic and visual interface for building and testing ETL scripts for data preparation. To use it in Amazon SageMaker Studio, the SageMaker execution role needs to have the necessary permissions for AWS Glue. This can be achieved by attaching the AwsGlueSessionUserRestrictedServiceRole managed policy to the SageMaker execution role.
* The method by which Redshift distributes the data across the nodes is called the distribution style. There are three types of distribution styles in Redshift: KEY, ALL, and EVEN.
  * KEY distribution style distributes the data across the nodes based on the values in one column. The rows with the same value in the distribution key column are located on the same node. This is beneficial when two tables are frequently joined on the distribution key column, as it allows for fast, local joins.
  * ALL distribution style replicates the entire table on every node. This is beneficial for small tables that are frequently joined with large tables, as it allows for fast, local joins. However, it uses a lot of storage space and can slow down data loading and update operations.
  * The EVEN distribution style (the default) distributes the rows evenly across all nodes without considering the join patterns. This could lead to a lot of network I/O during the join operations, which could slow down the query performance.
* You can use AWS Safe Maker Feature Store in the following modes:
  * Online – In online mode, features are read with low latency (milliseconds) reads and used for high throughput predictions. This mode requires a feature group to be stored in an online store.
  * Offline – In offline mode, large streams of data are fed to an offline store, which can be used for training and batch inference. This mode requires a feature group to be stored in an offline store. The offline store uses your S3 bucket for storage and can also fetch data using Athena queries.
  * Online and Offline – This includes both online and offline modes.
* Amazon CloudWatch Container Insights is a service that monitors, troubleshoots, and alarms on containerized applications and microservices. It collects metrics and logs from EKS, ECS, and Kubernetes clusters and provides automatic dashboards in the CloudWatch console for end-to-end visibility.
* The Amazon OpenSearch Service is a fully managed service that makes it easy to deploy, operate, and scale OpenSearch implementations. It provides enhanced security, availability, integration with other AWS services, and lower costs compared to managing your own OpenSearch cluster. Furthermore, it enables the visualization of log data through Kibana dashboards.
* In AWS EMR the MSCK REPAIR TABLE command is used to synchronize the metadata of a table with the actual data layout in the file system. When new data is added directly to HDFS, Hive is not aware of the new partitions. This is because Hive requires metadata about partitions to be updated in its metastore, which doesn’t happen when data is added directly to HDFS.
* Amazon S3 Access Points are a feature of Amazon S3 that simplifies managing data access for shared datasets. With Access Points, you can create multiple access points for a single S3 bucket, with each access point having a unique hostname and access policy tailored for a specific use case or application.
* Amazon Redshift is a cloud-based data warehouse that you can use to perform data analysis. One of the features of Amazon Redshift is the support for the MERGE operation, which uses a temporary staging table. This operation is particularly useful when you have a large batch of updated data that often contains updates to existing records, and you want to ensure that the data in Redshift is always up-to-date while minimizing the impact on query performance. The MERGE operation works by comparing the records in two tables based on a specified match condition. When the MERGE operation detects a match between the records in the two tables, it performs an UPDATE operation. This means that the existing records in the main table are updated with the values from the matching records in the temporary table. On the other hand, when the MERGE operation does not detect a match, it performs an INSERT operation. This means that the new records from the temporary table are inserted into the main table.
* In Amazon Redshift, stored procedures serve as a mechanism to encapsulate logic for various operations, such as data transformation and validation, as well as business-specific logic. This encapsulation allows for a more streamlined and efficient process, particularly when dealing with multiple SQL statements.
* Amazon Redshift’s Concurrency Scaling feature allows you to automatically add additional cluster capacity to handle an increase in concurrent queries. By configuring a separate WLM queue for the long-running query, you can ensure that this query does not block other queries from executing. 
* AWS Glue offers different engines on which you can run jobs, providing flexibility for various data processing needs:
  * Spark: AWS Glue on Apache Spark is designed for complex ETL jobs that require processing large volumes of data. Spark’s distributed computing model enables high-performance data transformation and analysis across clusters of computers. It’s ideal for tasks that need to handle big data sets, perform extensive data manipulation, or require the use of Spark’s rich ecosystem of libraries.
  * Ray: AWS Glue for Ray is optimized for highly parallel and compute-intensive tasks. Ray is a framework that allows for easy scaling of applications from a laptop to a cluster, making it suitable for machine learning workflows, real-time data processing, and tasks that require concurrency.
  * Python Shell: AWS Glue’s Python Shell is best suited for light to medium data transformation tasks that do not necessitate the distributed computing capabilities of Spark or Ray. It allows for simpler, script-based ETL jobs that can be executed without the overhead of setting up a Spark or Ray environment.
* Apache Iceberg is an open table format for very large analytic datasets. Iceberg manages large collections of files as tables, and it supports modern analytical data lake operations such as record-level insert, update, delete, and time travel queries. The Iceberg specification allows seamless table evolution, such as schema and partition evolution, and is designed for optimized usage on Amazon S3.
* Amazon Redshift Serverless enables the use of Redshift capabilities without the need to provision and manage Redshift clusters, which is ideal for workloads that are intermittent and short-lived.
* Amazon SageMaker Data Wrangler reduces the time it takes to aggregate and prepare data for machine learning (ML) from weeks to minutes. With SageMaker Data Wrangler, you can simplify the process of data preparation and feature engineering, and complete each step of the data preparation workflow, including data selection, cleansing, exploration, and visualization from a single visual interface. After the exploratory data analysis (EDA) is complete, exporting the results to Amazon S3 for further processing aligns with the requirements of securely storing and retrieving data for predictive modeling. 
* AWS Glue Catalog resource policies allow you to manage access at a granular level. You can specify permissions at the database, table, and column level. 
* Leveraging Amazon Aurora’s zero-ETL integration with Amazon Redshift allows direct analysis of the data without the need for ETL jobs. This feature enables near real-time analytics and machine learning on petabytes of transactional data.
* Amazon Redshift, a fully managed, petabyte-scale data warehouse service, allows you to create custom scalar UDFs using either a SQL SELECT clause or a Python program. Once created, the UDF is stored in the database and is available for any user with sufficient privileges to run.
* AWS Glue crawlers automatically detect schemas for data sources. Glue crawlers scan data sources, detect schemas, and store the associated metadata in the AWS Glue Data Catalog. This enables Glue to handle changing schemas without requiring code changes. The crawlers run periodically to detect new or changed data.
* A COPY command is the most efficient way to load a table. You can also add data to your tables using INSERT commands, though it is much less efficient than using COPY. The COPY command is able to read from multiple data files or multiple data streams simultaneously. Amazon Redshift allocates the workload to the cluster nodes and performs the load operations in parallel, including sorting the rows and distributing data across node slices. You can use temporary staging tables to hold the data for transformation. These tables are automatically dropped after the ETL session is complete. Temporary tables can be created using the CREATE TEMPORARY TABLE syntax, or by issuing a SELECT … INTO #TEMP_TABLE query. Explicitly specifying the CREATE TEMPORARY TABLE statement allows you to control the DISTRIBUTION KEY, SORT KEY, and compression settings to further improve performance.
* AWS Glue Flex is designed for non-urgent data integration workloads that do not require fast job startup times or consistent runtimes. It runs on spare compute capacity instead of dedicated hardware, which can lead to variable start and runtimes but is more cost-effective.
* AWS Glue’s support for server-side filtering with catalog partition predicates directly during the creation of DynamicFrames is a powerful feature for optimizing ETL processes. This capability allows the ETL job to selectively process only the necessary data by utilizing the metadata catalog’s partition indexes. Doing so significantly reduces the volume of data that needs to be read and processed, leading to reductions in execution time and cost.
* AWS Glue provides a feature called job metrics, which can be used to estimate the number of DPUs that can be used to scale out an AWS Glue job. This feature is particularly useful in understanding the resource utilization of your jobs and can help in making informed decisions about scaling.
* Enhanced Fan-Out is a feature in Kinesis Data Streams that provides each consumer with its own 2 MB per second of dedicated bandwidth per shard, which can help handle increased data volume. Additionally, the HTTP/2 data retrieval API can reduce latency, making data delivery from producers to consumers faster.
* Amazon Redshift supports querying nested data in JSON columns using dot and bracket notation. This allows you to access and query nested data efficiently, which is especially useful when dealing with large datasets.
* The UNLOAD command in Amazon Redshift allows you to unload the result of a query to Amazon S3. This command is particularly useful for exporting data that is not frequently accessed, thereby saving storage space in Redshift and reducing costs.
* Amazon Redshift offers several features to enhance query performance, such as result caching, concurrency scaling, and materialized views. Materialized views in Redshift provide a powerful way to optimize query performance for repeated and predictable query workloads. They precompute and store the results of a query, which improves the speed of data retrieval for complex queries that are run frequently. 
* Amazon SQS is a fully managed message queuing service that provides a virtual queue acting as a buffer between the components of an application. It enables you to decouple and scale microservices, distributed systems, and serverless applications.
  * DelaySeconds setting refers to the delay period for a delay queue. This sets how long any messages sent to the queue remain invisible to consumers after being added to the queue. The default is 0 seconds, and the maximum is 15 minutes.
  * VisibilityTimeout is the period of time during which Amazon SQS prevents other consuming components from receiving and processing a message after it has been received from a queue (but not yet deleted). This helps prevent a message from being received and processed more than once.
  * MaxReceiveCount refers to the number of times a message can be received from the queue before being deleted. When the limit is reached, the message will be sent to the dead letter queue. The default Maximum received is 10.
  * ReceiveMessageWaitTimeSeconds is an optional parameter that sets the duration (in seconds) for waiting for a message to arrive in the queue.
* AWS Glue provides real-time, continuous logging for AWS Glue jobs. You can view real-time Apache Spark job logs in Amazon CloudWatch, including driver logs, executor logs, and an Apache Spark job progress bar. Viewing real-time logs provides you with a better perspective on the running job.
* In AWS Step Functions, the Parallel state is used to create parallel branches of execution in your state machine
* Materialized views in Amazon Redshift provide a way to handle complex queries on large datasets. A materialized view contains a precomputed result set, based on an SQL query over one or more base tables. This means that the results of the query are calculated and stored when the view is created, rather than being calculated at query time. This can significantly speed up query performance on large datasets.
* EXECUTE permission allows users to run a stored procedure. This is the main permission required to allow users to execute procedures. GRANT EXECUTE is used to provide this permission.
* UltraWarm nodes in Amazon OpenSearch Service utilize Amazon Simple Storage Service (Amazon S3) along with associated caching solutions to enhance performance. UltraWarm provides a significantly lower cost per GiB for read-only data that is queried less frequently and doesn’t require the same performance level as hot storage.
* Amazon Athena Notebooks is an interactive coding environment where you can write and execute Apache Spark code using Python. Athena notebooks allow chaining together SQL queries, calculations, and visualizations.
* Amazon Redshift keeps a record of all connections and user activities in your database, which is commonly known as database auditing. These logs are essential for security and troubleshooting purposes. They are stored in Amazon S3 buckets, which provide easy access and data security features for those who monitor database activities.
* Amazon EventBridge Pipes connects sources to targets. Pipes are intended for point-to-point integrations between supported sources and targets, with support for advanced transformations and enrichment. 
* In Amazon Athena, a view is a logical, not a physical table. It’s essentially a saved query that you can reference, just like a table. This is particularly useful when dealing with complex queries.
* Amazon Athena Workgroups allows you to separate your query execution across different teams, workloads, and applications or even run them in isolation from other queries. Workgroups provide a way to track query-related metrics, set limits to manage costs and enforce settings across all queries within the workgroup. Each workgroup has its own query history, metrics, data usage, and performance statistics, allowing for better resource allocation and cost management.
* AWS Glue connection is a Data Catalog object that stores login credentials, URI strings, virtual private cloud (VPC) information, and more for a particular data store. AWS Glue crawlers, jobs, and development endpoints use connections in order to access certain types of data stores. You can use connections for both sources and targets and reuse the same connection across multiple crawler or extract, transform, and load (ETL) jobs.
* AWS Glue workflows provide a visual interface for orchestrating complex extract, transform, and load (ETL) activities involving multiple crawlers, jobs, and triggers. Each workflow manages the execution and monitoring of all its jobs and crawlers. As a workflow runs each component, it records execution progress and status. This provides you with an overview of the larger task and the details of each step.
* An Athena workgroup is a feature of Amazon Athena that allows for the grouping of queries. By default, each account has a primary workgroup, and the default permissions allow all authenticated users access to this workgroup. Each workgroup that you create shows saved queries and query history only for queries that ran in it. 
* Amazon Redshift Data API is an Amazon Redshift feature that simplifies access to your Amazon Redshift data warehouse by removing the need to manage database drivers, connections, network configurations, data buffering, credentials, and more. 
* The concurrency scaling feature in Amazon Redshift automatically adds and removes capacity by adding concurrency scaling to handle demands from thousands of concurrent users, thereby providing consistent SLAs for unpredictable and spiky workloads such as BI reports, dashboards, and other analytics workloads.