# AWS Certified Data Engineer Associate

* AWS Lake Formation is a service that simplifies the management of security and governance for data lakes. It offers centralized, granular access control to data across multiple AWS services like Amazon S3 and AWS Glue Data Catalog. By using Lake Formation, the administrator can manage permissions in a unified way, which is beneficial for large data lakes with numerous data sources and metadata.
* AWS Glue Studio offers a graphical interface that simplifies the creation, execution, and monitoring of ETL (Extract, Transform, Load) jobs in AWS Glue. This visual interface is especially valuable for teams with varying levels of technical expertise, enabling users to visually compose data transformation workflows.
* The MSCK REPAIR TABLE command in Amazon Athena is specifically designed to address the issue of Athena not returning results from newly added data partitions. This command works by scanning the specified directory in Amazon S3, which is linked to the Athena table, to discover and add any new Hive-compatible partitions. This automatic update of the table metadata in Athena is essential for ensuring that the query engine recognizes and includes newly added data partitions in its query results. The command is particularly beneficial in environments where data partitions in S3 are frequently updated or added, as it streamlines the synchronization process between the data storage in S3 and the metadata in Athena, thereby facilitating accurate and up-to-date query results.
* The AWS Glue FindMatches ML transform, part of the AWS Glue service, is a machine learning tool adept at identifying and associating records that refer to the same entity, especially when a common unique identifier is absent. This process involves a “teaching” approach where users label example records as matches or non-matches. AWS Glue leverages these labeled examples to understand patterns in data, enabling it to recognize similar records.
* Amazon Athena now supports User Defined Functions (UDFs), allowing customers to write custom scalar functions and invoke them in SQL queries. UDFs in Athena are defined within an AWS Lambda function as methods in a Java deployment package. This feature is particularly useful for scenarios requiring custom processing, such as categorizing earthquake locations using a geospatial indexing system. UDFs enable the encapsulation of complex logic into reusable functions, thus simplifying SQL queries in Athena.
* AWS Glue Data Quality, as a component of AWS Glue, automates the identification of data quality issues. It scans the data for common problems such as inconsistencies, missing values, or format errors. This feature is crucial in healthcare scenarios where data must be accurate and consistent. For example, in the case of patient records, AWS Glue Data Quality can automatically detect irregularities like mismatched date formats or incomplete patient entries.
* Amazon Athena’s “Reuse Query Results” feature allows users to reuse the last stored query result when re-running a query, enhancing performance and reducing costs. This is especially beneficial when the underlying data does not change within a certain time frame, like in weekly sales report generation. 
* Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. This feature is highly effective in converting query results into more efficient storage formats, such as Parquet or ORC, which are open source columnar formats. These formats are optimized for Athena, leading to reduced data scanning per query, thereby enhancing query performance and lowering costs. Athena does not support transaction-based operations on table data, ensuring that operations like CREATE, UPDATE, or DELETE are ACID-compliant. When using CTAS, the format property can specify storage formats like ORC, PARQUET, AVRO, JSON, or TEXTFILE, with the default being PARQUET. Additionally, for formats like PARQUET and ORC, compression formats like SNAPPY can be specified using the write_compression property. This approach of transforming data into efficient formats directly caters to the need for improved query performance due to increased data volume and complexity.
* Amazon Athena Federated Query is a powerful feature that allows SQL queries to be executed across multiple data sources without the need for data movement. This service is particularly useful for scenarios where data resides in different storage systems and formats, as it enables querying data in place. With Athena Federated Query, you can extend the scope of your SQL queries to not just Amazon S3 but also to other data sources such as Amazon Aurora MySQL, HBase on Amazon EMR, and Amazon DynamoDB. This capability is crucial for creating comprehensive reports from various data sources without the overhead of extracting, transforming, and loading (ETL) the data into a single repository.
* Amazon Managed Workflows for Apache Airflow is a managed orchestration service for Apache Airflow that you can use to set up and operate data pipelines in the cloud at scale. Apache Airflow is an open-source tool used to programmatically author, schedule, and monitor sequences of processes and tasks referred to as workflows.
* You can use the SSHOperator in a directed acyclic graph (DAG) to connect to a remote Amazon EC2 instance from your Amazon Managed Workflows for Apache Airflow environment. You can also use a similar approach to connect to any remote instance with SSH access. You have to install the apache-airflow-providers-ssh package on the web server via the requirements.txt file.
* Amazon AppFlow is a fully-managed integration service that enables you to securely exchange data between software as a service (SaaS) applications, such as Salesforce, and AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Redshift. For example, you can ingest contact records from Salesforce to Amazon Redshift or pull support tickets from Zendesk to an Amazon S3 bucket.
* Amazon Data Firehose can convert the format of your input data from JSON to Apache Parquet or Apache ORC before storing the data in Amazon S3. Parquet and ORC are columnar data formats that save space and enable faster queries compared to row-oriented formats like JSON. 
* Amazon Macie is a security service that utilizes machine learning algorithms to automatically identify, classify, and secure sensitive data in AWS. Amazon Macie protects sensitive data, such as personally identifiable information (PII) (e.g., names, addresses, credit card numbers, and intellectual property), from unauthorized access. After identifying sensitive data, Macie generates detailed reports and alerts, which can be integrated with Amazon EventBridge for automated response actions.
* With AWS Glue, you only pay for the time your ETL job takes to run. You are charged an hourly rate based on the number of Data Processing Units (DPU) to run your ETL job. A Data Processing Unit can also be referred to as a worker, which is the processing power of your ETL job.
* Amazon SageMaker ML Lineage Tracking offers a way to create and store information about the various steps in an ML workflow. This feature is integral for establishing model governance and audit standards, which are essential for ensuring the accuracy, completeness, and trustworthiness of the data used in ML decisions.
* AWS Lake Formation is the optimal choice for setting up a centralized metadata storage solution that demands fine-grained access control. Utilizing Lake Formation data filters, a data engineer can enforce security measures at the database, table, column, row, and cell levels. This capability, coupled with its scalability and reliability, makes Lake Formation the ideal solution with minimal operational overhead.
* In Amazon SQS, several events can lead to the removal of messages, which is crucial for ensuring the queue’s efficiency and reliability. A DeleteMessage API call is a direct method to remove a message from the queue, typically after it has been processed by a consumer. This ensures that messages are not processed more than necessary. Reaching the maxReceiveCount for a message is another way messages are removed; this occurs when a message has been received a specified number of times but not deleted, indicating processing issues and usually results in the message being sent to a dead-letter queue. Furthermore, performing a purge operation on the queue instantly clears all messages, useful for resetting or troubleshooting the queue.
* Substitution is a data-masking technique that involves replacing actual PII data with other authentic-looking values. This technique is useful in maintaining the overall structure and statistical distribution of the data, which is important for testing or development purposes.
* When you select incremental transfer, Amazon AppFlow transfers only the records that have been added or changed since the last successful flow run. This is achieved by selecting a source timestamp field to specify how Amazon AppFlow identifies new or changed records. For example, if you have a Created Date timestamp field, you can instruct Amazon AppFlow to transfer only newly-created records (and not changed records) since the last successful flow run.
* Amazon Managed Streaming for Apache Kafka (Amazon MSK) is a fully managed service that enables you to build and run applications that use Apache Kafka to process streaming data
* Apache Kafka’s Access Control Lists (ACLs) offer a highly granular approach to permissions management. They provide a mechanism to regulate which applications can read or write to a certain topic.
* AWS CloudTrail Lake simplifies activity log analysis by integrating collection, storage, optimization, and query in the same product. By consolidating these features into one environment, CloudTrail Lake eliminates the need for separate data processing pipelines that span across teams and products.
* AWS Glue DataBrew offers a streamlined solution for data quality management, particularly beneficial in scenarios requiring precise and automated data validation. Its ability to define specific data quality rules within a ruleset makes it an optimal choice for scenarios like ensuring inventory data accuracy. 
* Amazon Redshift Spectrum stands out as a highly efficient solution for querying data across Amazon S3 and Amazon Redshift. It leverages the benefits of low-cost storage and open data formats, along with Redshift’s advanced query optimization and fast access to local disks. Redshift Spectrum’s serverless architecture aligns with Amazon Athena’s, negating the need for resource provisioning and management.
* AWS Glue offers a Job run monitoring feature that gives a quick overview of job runs, including status, timestamps, and other key details. For deeper insights, the AWS Glue Job Profiler goes a step further, offering detailed metrics such as execution time, data processing rates, and memory usage. These metrics are crucial for pinpointing performance bottlenecks and fine-tuning resource utilization in ETL jobs.
* Amazon S3 Transfer Acceleration is a bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket. It takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. This results in faster data transfers and reduced latency, which is crucial for the scenario’s requirement of fast access retrieval times.
* Amazon Redshift has two types of nodes: leader nodes and compute nodes.
  * Compute Nodes: These nodes store and process the data. They perform the heavy lifting of running queries, filtering data, and performing computations.
  * Leader Node: This node manages the compute nodes, receives queries, and distributes the workload across the compute nodes. It also manages external communication and aggregates the results from the compute nodes. Certain functions, including current_schema(), are leader node–only functions, meaning they can only be executed on the leader node.
And two types of tables:  
  * System Tables: Redshift has several system tables that store metadata and information about the cluster, databases, schemas, tables, and users. These tables are typically prefixed with “PG_” or “STV_”.
  * User-Defined Tables: These are the tables created by users to store their data in Redshift.
* AWS Database Migration Service(AWS DMS) is a cloud service that makes it possible to migrate relational database, data warehouses, NoSQL databases, and other data stores. You can use AWS DMS to transfer data to Amazon S3 from any database source. When Amazon S3 is employed as a target in an AWS DMS task, both full load and change data capture(CDC) data are written to a comma-separated value (.csv) format by default. However, to ensure more compact storage and faster query performance, you may opt to have the data written in Apache Parquet(.parquet) format.
* The Athena JDBC Connector allows connecting BI tools, reporting tools or custom applications to Amazon Athena using standard JDBC. This enables running SQL queries and fetching results from Athena tables using common SQL tools without needing to use AWS CLI or SDKs.
* AWS Glue FindMatches Machine Learning (ML) Transform enables you to identify duplicate or matching records in your dataset, even when the records do not have a common unique identifier and no fields match exactly. This will not require writing any code or knowing how machine learning works. FindMatches can be useful in many different problems, such as:
  * Matching customers: Linking customer records across different customer databases, even when many customer fields do not match exactly across the databases (e.g., different name spelling, address differences, missing or inaccurate data, etc).
  * Matching products: Matching products in your catalog against other product sources, such as product catalog against a competitor’s catalog, where entries are structured differently.
  * Improving fraud detection: Identifying duplicate customer accounts, determining when a newly created account is (or might be) a match for a previously known fraudulent user.
  * Other matching problems: Match addresses, movies, parts lists, etc. In general, if a human being could look at your database rows and determine that they were a match, there is a really good chance that the FindMatches transform can help you.
* Amazon SageMaker is a service that simplifies building, training, and deploying machine learning models. SageMaker is a platform that facilitates the creation of high-quality machine learning models by abstracting away the complexity typically associated with each process step. It offers an integrated Jupyter Notebook environment for data exploration and preprocessing, optimized and ready-to-use algorithms, and a fully managed deployment service. This enables practitioners to focus more on solving the problem than on the operational overhead of machine learning pipelines.
* SageMaker Canvas is a feature of Amazon SageMaker that represents a no-code approach to machine learning. It makes machine learning accessible to users of all technical backgrounds. With a user-friendly interface, it automates data preparation, model selection, training, and deployment. This allows users to create predictive models without needing to write any code. Canvas simplifies the entire process of applying machine learning, from data cleaning to making predictions.
* The Lock command in Redshift restricts access to the database table. This command enables the acquisition of a table-level lock in “ACCESS EXCLUSIVE” mode. It waits, if necessary, for any conflicting locks to be released. Explicitly locking a table in this way causes other transactions or sessions to wait when attempting to read or write to the table.
* Amazon Redshift has three lock modes:
  * AccessExclusiveLock: Acquired primarily during DDL operations, such as ALTER TABLE, DROP, or TRUNCATE. AccessExclusiveLock blocks all other locking attempts.
  * AccessShareLock: Acquired during UNLOAD, SELECT, UPDATE, or DELETE operations. AccessShareLock blocks only AccessExclusiveLock attempts. AccessShareLock doesn’t block other sessions that are trying to read or write on the table.
  * ShareRowExclusiveLock: Acquired during COPY, INSERT, UPDATE, or DELETE operations. ShareRowExclusiveLock blocks AccessExclusiveLock and other ShareRowExclusiveLock attempts but doesn’t block AccessShareLock attempts.
* By enabling partition projection, Athena can bypass the traditional metadata loading process. Instead, it will directly calculate which partition to access based on the query criteria using rules and patterns specified in the table definition.
* Amazon Athena Federated Query allows you to query data from various sources using AWS Lambda-based connectors. The connectors integrate with data sources like Amazon CloudWatch Logs, DynamoDB, and relational databases like MySQL, enabling SQL queries to span different data locations.
* Amazon AppFlow is a fully managed integration service that enables secure, private data transfer between AWS services and SaaS applications with minimal operational overhead. It allows for the direct integration of SaaS applications like Salesforce, ServiceNow, Slack, and many others with AWS services.
* Amazon Redshift service allows users to create and manage materialized views, which are stored or cached views of a query’s result set for faster access. Redshift supports various automation options, including AWS services and external tools, to automate the updates of these views.
* The query editor v2 in Amazon Redshift can execute SQL commands directly within the Redshift console, including commands to refresh materialized views. These commands can be scheduled to execute automatically and can be set up directly in the Redshift environment. This provides a straightforward method to automate materialized view updates without additional infrastructure or services, thereby minimizing operational overhead.
* AWS Step Functions allows the coordination of multiple AWS services into serverless workflows. By designing a state machine, AWS Step Functions can manage the execution order, dependency tracking, and error handling of the tasks
* The AWS Glue Data Catalog is a fully managed, serverless, and cloud-optimized metadata repository that integrates seamlessly with other AWS services. It provides a unified view of all your data assets, making it easier to discover, search, and manage data across various AWS data stores. AWS Glue crawlers automatically scan your data sources, extract schema information, and populate the Glue Data Catalog with metadata tables, which can be used for data discovery, extract, transform, and load (ETL) jobs, and query execution. This process significantly reduces manual efforts and ensures that your data catalog is always up-to-date with the latest schema changes.
* The AWS Glue Data Catalog provides a centralized metadata repository for clients running Apache Hive and Apache Spark. This serverless repository eliminates the need for operational tasks like scaling, patching, and managing servers, which aligns well with the organization’s goal to adopt a serverless architecture. The AWS Glue Data Catalog can seamlessly integrate with Amazon EMR, providing a managed Hive metastore service that simplifies the process of cataloging, searching, and querying metadata.
* Data distribution in Amazon Redshift refers to how data is stored across the nodes and slices within a cluster. When you load data into a Redshift table, the rows are distributed and stored across these slices according to the table’s distribution style. KEY distribution styles: The rows are distributed based on the values in a specified distribution key column. Rows with the same value are stored together. This helps optimize queries that filter or join on the distribution key column. The different key concepts in Amazon Redshift are:
  * Distribution key: This column is used to distribute the rows across nodes in the cluster. It is specified when creating a table using the DISTKEY clause.
  * Sort key: This column is used to order or sort the rows stored on each node. It is specified using the SORTKEY clause. Multiple columns can be used as sort keys.
  * Primary key: This uniquely identifies each row in the table. It consists of the distribution key as the partition key and can optionally include a sort key.
* The AWS Glue Schema Registry tracks schema changes to tables in the data catalog. It allows schemas to evolve over time while maintaining compatibility with existing analytics, queries, and applications. This enables schema-on-read for analytics tools like Athena, EMR, and Redshift Spectrum.
* AWS CloudTrail’s management events provide visibility into management or control plane operations that are performed on AWS resources, such as creating, modifying, or deleting resources. This includes API calls and console sign-ins. Management events are enabled by default when you create a trail.
* AWS CloudTrail’s data events (also known as “data plane operations”) provide insights into resource or data plane operations performed within resources, such as S3 object uploads and downloads or Lambda function executions. These are often high-volume activities. Data events are disabled by default, and you need to explicitly configure the trail to record them for supported resources. Recording data events incur additional costs.
* AWS Lake Formation is a central hub on Amazon S3 for storing structured and unstructured data until needed. It manages permissions and access for data stored in S3. Lake Formation provides granular control over data access and a unified Storage API for secure data operations. Key features include encryption, private endpoints, and audit logs for enhanced security and isolation of the data lake within S3.
* Converting CSV data into a columnar format such as Apache Parquet significantly improves performance. This conversion enables Athena to process queries more efficiently by reading only the necessary columns, thereby reducing the amount of data scanned and lowering costs. Secondly, ensuring that the AWS region for the S3 data storage matches the region where Athena queries are executed minimizes network latency, leading to faster query performances.
* VACUUM is a command in Amazon Redshift that optimizes query performance by reclaiming disk space from deleted or updated rows, rebuilding indexes, and reorganizing tables. The main VACUUM commands are: 
  * VACUUM FULL – Reclaims disk space, rebuilds indexes, and re-sorts all rows in the table. This is the default VACUUM operation in Redshift.
  * VACUUM DELETE ONLY – Only reclaims disk space occupied by deleted rows without re-sorting data. This is faster than VACUUM FULL but does not optimize query performance.
  * VACUUM REINDEX – Analyzes the distribution of values in interleaved sort key columns and performs a full VACUUM operation, including re-sorting. Used when interleaved sort keys are skewed.
  * VACUUM SORT ONLY – This command performs a sort-only operation on the table without reclaiming disk space. It is useful when rows are unsorted, but space is not an issue.
Amazon Redshift automatically performs VACUUM DELETE ONLY operations in the background to reclaim space from deleted rows.
* You can enable Amazon Athena’s query result caching feature.
* A DataBrew recipe is a set of data transformation steps that are applied sequentially to clean, normalize, and prepare data. Here are some of those recipe steps:
  * NEST_TO_ARRAY converts selected columns into an array while maintaining their order and typecasting them into a common data type.
  * NEST_TO_MAP converts user-selected columns into key-value pairs, with each key representing the column name and the corresponding row value as the value. Column order is not preserved, and different column data types are typecast to a common type.
  * UNNEST_MAP separates a map column into key-value pairs, generating a row for each pair. It only works for one map column level.
  * UNNEST_ARRAY function extracts array elements into separate rows, allowing only one level of an array column to be unnested.
* Amazon Redshift Streaming Ingestion allows ingesting streaming data from Kinesis data streams into an Amazon Redshift data warehouse in near real-time for analytics. It can ingest hundreds of megabytes of data per second with low latency.
* AWS Glue Interactive Sessions provides a programmatic and visual interface for building and testing ETL scripts for data preparation. To use it in Amazon SageMaker Studio, the SageMaker execution role needs to have the necessary permissions for AWS Glue. This can be achieved by attaching the AwsGlueSessionUserRestrictedServiceRole managed policy to the SageMaker execution role.
* 